{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling & pre-processing\n",
    "\n",
    "Data pre-processing is a necessary step in data analysis. It is the process of converting or mapping data from one raw form into another format to make it ready for further analysis.\n",
    "\n",
    "Data pre processing is often called data cleaning or data wrangling, and there are likely other terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values\n",
    "\n",
    "Usually, missing value in dataset appears as question mark (?), N/A, zero (0) or just a blank cell().\n",
    "\n",
    "There are many ways to deal with missing values and this is regardless of Python, R or whatever tool you use. Of course, each situation is different and should be judged differently. However, these are the typical options you can consider.\n",
    "\n",
    "1. The first is to check if the person or group that collected the data can go back and find what the actual value should be.\n",
    "\n",
    "2. Another possibility is just to remove the data where that missing value is found. When you drop data, you could either drop the whole variable or just the single data entry with the missing value. If you don't have a lot of observations with missing data, usually dropping the particular entry is the best.\n",
    "\n",
    "3. If you're removing data, you want to look to do something that has the least amount of impact. Replacing data is better since no data is wasted. However, it's less accurate since we need to replace missing data with a guess of what the data should be. One standard replacement technique is to replace missing values by the average value of the entire variable.\n",
    "\n",
    "Handling missing data\n",
    "1. Delete the missing values (if not more than 30% of the total dataset)\n",
    "2. Impute missing data with zeros (if certain there was no information)\n",
    "3. Impute with mean (if numerical values are normally distributed)\n",
    "4. Impute with median (if numerical values are not normally distributed)\n",
    "5. Impute with mode (for categorical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "\n",
    "Data is usually collected from different places, by different people, which may be stored in different formats. Data formatting means bringing data into a common standard of expression that allows users to make meaningful comparisons.\n",
    "\n",
    "As a part of dataset cleaning, data formatting ensures that data is consistent and easily understandable.\n",
    "\n",
    "For example, people may use different expressions to represent New York City, such as N.Y., Ny, NY, and New York. Sometimes this unclean data is a good thing to see.But perhaps, more often than not, we just simply want to treat them all as the same entity or format to make statistical analysis easier down the road.\n",
    "\n",
    "Referring to our used car dataset, there's a feature named city-miles per gallon in the dataset, which refers to a car fuel consumption in miles per gallon unit. However, you may be someone who lives in a country that uses metric units, so you would want to convert those values to liters per 100 kilometers, the metric version. To transform miles per gallon to liters per 100 kilometers.\n",
    "\n",
    "For a number of reasons, including when you imported dataset into Python, the data type may be incorrectly established.  It is important for later analysis to explore the feature's data type and convert them to the correct data types. Otherwise, the developed models later on may behave strangely and totally valid data may end up being treated like missing data.\n",
    "\n",
    "To identify a feature's data type in Python, we can use the dataframe.dtypes method and check the data type of each variable in a data frame. In the case of wrong data types, the method dataframe.astype can be used to convert a data type from one format to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "#### There are several ways to normalize data. \n",
    "\n",
    "1. The first method, called simple feature scaling, just divides each value by the maximum value for that feature. This makes the new values range between 0 and 1.\n",
    "\n",
    "2. The second method, called min max, takes each value x underscore old, subtracts it from the minimum value of that feature, then divides by the range of that feature. Again, the resulting new values range between 0 and 1.\n",
    "\n",
    "3. The third method is called Z-score, or standard score. In this formula, for each value, you subtract the mu, which is the average of the feature and then divide by the standard deviation sigma. The resulting values hover around zero and typically range between negative three and positive three, but can be higher or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning\n",
    "This ia also a method of data preprocessing.\n",
    "\n",
    "1. Binning is when you group values together into bins. For example, you can bin age into 0-5, 6-10, 11-15 and so on.\n",
    "\n",
    "2. Binning can improve accuracy of the predictive models.\n",
    "\n",
    "3. Data binning is used to group a set of numerical values into a smaller number of bins to have a better understanding of the data distribution.\n",
    "\n",
    "4. You can then use histograms to visualize the distribution of the data after they've been divided into bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Categorical Variables into Quantitative Variables\n",
    "\n",
    "Most statistical models cannot take in objects or strings as input, and for model training, only take the numbers as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Summary\n",
    "\n",
    "At this point, you know:\n",
    "\n",
    "Data formatting is critical for making data from various sources consistent and comparable.\n",
    "\n",
    "Master the techniques in Python to convert units of measurement, like transforming \"city miles per gallon\" to \"city-liters per 100 kilometers\" for ease of comparison and analysis.\n",
    "\n",
    "Acquire skills to identify and correct data types in Python, ensuring the data is accurately represented for subsequent statistical analyses.\n",
    "\n",
    "Data normalization helps make variables comparable and helps eliminate inherent biases in statistical models.\n",
    "\n",
    "You can apply Feature Scaling, Min-Max, and Z-Score to normalize data and apply each technique in Python using pandas’ methods.\n",
    "\n",
    "Binning is a method of data pre-processing to improve model accuracy and data visualization.\n",
    "\n",
    "Run binning techniques in Python using numpy's \"linspace\" and pandas' \"cut\" methods, particularly for numerical variables like \"price.\"\n",
    "\n",
    "Utilize histograms to visualize the distribution of binned data and gain insights into feature distributions.\n",
    "\n",
    "Statistical models generally require numerical inputs, making it necessary to convert categorical variables like \"fuel type\" into numerical formats.\n",
    "\n",
    "You can implement the one-hot encoding technique in Python using pandas’ get_dummies method to transform categorical variables into a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice the above concepts on the customer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing an excel\n",
    "df = pd.read_csv(\"Customer_Data.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the cleaned dataset\n",
    "#df.to_csv(\"path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
